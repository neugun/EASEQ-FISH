{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ransac affine for point cloud alignment of two masks\n",
    "In this tutorial we will learn to warp moving masks and examine point clouds (centriod of ROI) using two variants of the ransac affine and ICP algorithm. We can apply the ROI to extract images for later ROI_affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing for masks of moving rounds:\n",
    "1. 3d starfinity segmentation of fixed round DAPI (segmentation masks)\n",
    "2. filter ROI (remove non-neuron edge masks, merge over segmentation masks）\n",
    "3. apply warp/invtransform to fixed round DAPI mask\n",
    "4. remove noise on edge, mannually check the ROI shape\n",
    "\n",
    "Steps for compare 2 rounds of masks, and export ROI_SPOTS\n",
    "1. Extract ROI centriods as point cloud. Calculate the knn distance (20 pixels)\n",
    "2. Ransac affine the point sets (<10% outlier, distance is 20 pixels). \n",
    "    a. spot extraction, context(no images any more), remove outlier by calculating correlation, ransac affine\n",
    "    b. may add other information for the ROI_matching: # relative positions (distance to 3 nearest points), area (within 0.5-1.5), aspect_ratio.\n",
    "    b.Visualize the masks and points\n",
    "3. Use the ICP to minimize the distance of correponding ROIs. visualize the masks and points.Find the nearest ROIs using knn-neighbor. \n",
    "\n",
    "Bash\n",
    "#maybe not necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input: \n",
    "1)segmentation mask of fixed rounds, 2) invtransformation, 3)extracted spots, 4)ROI_intensity info\n",
    "#### output: \n",
    "1) segmentation mask of moving rounds, 2)point clouds of masks(center of each mask), 3) ROI index and meata,4) ROI_spots; 5) distance of warped ROI and real ROI (from the segmentation). 5. Generate excel for corresponding ROIs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pools\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "from math import pi, sin, cos, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import cm\n",
    "from scipy import ndimage\n",
    "import scipy.io\n",
    "from skimage import data\n",
    "from skimage.io import imread, imsave\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "from cv2 import estimateTranslation3D\n",
    "import tifffile\n",
    "import seaborn as sns\n",
    "from skimage.measure import regionprops\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# import bigstream library\n",
    "import zarr\n",
    "from bigstream import ransac\n",
    "from bigstream import transform\n",
    "\n",
    "# napari\n",
    "%gui qt5\n",
    "import napari\n",
    "# viewer = napari.view_image(data.astronaut(), rgb=True)\n",
    "# napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with loading the required modules for ransac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_align_points(\n",
    "    pA, pB, threshold, diagonal_constraint=0.75, default=np.eye(4)[:3],\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # sensible requirement of 51 or more spots to compute ransac affine\n",
    "    if len(pA) <= 10 or len(pB) <= 10:\n",
    "        if default is not None:\n",
    "            print(\"Insufficient spot matches for ransac, returning default identity\")\n",
    "            return default\n",
    "        else:\n",
    "            raise ValueError(\"Insufficient spot matches for ransac, need more than 10\")\n",
    "\n",
    "    # compute the affine\n",
    "    r, Aff, inline = cv2.estimateAffine3D(pA, pB, ransacThreshold=threshold, confidence=0.999)\n",
    "\n",
    "    print(np.diag(Aff))\n",
    "    print(Aff)\n",
    "    print(diagonal_constraint)\n",
    "    # rarely ransac just doesn't work (depends on data and parameters)\n",
    "    # sensible choices for hard constraints on the affine matrix\n",
    "    if np.any( np.diag(Aff) < diagonal_constraint ):\n",
    "        if default is not None:\n",
    "            print(\"Degenerate affine produced, returning default identity\")\n",
    "            return default\n",
    "        else:\n",
    "            raise ValueError(\"Degenerate affine produced, ransac failed\")\n",
    "\n",
    "    return Aff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocalization filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colocalization(spot_c0,spot_c1,neighbor_radius):\n",
    "    # Fix and Mov true spots\n",
    "#     neighbor_radius = math.sqrt(3*3*3) #3.464\n",
    "    #vox=[0.23,0.23,0.38]\n",
    "    c0=spot_c0[:,:3].copy()\n",
    "    c1=spot_c1[:,:3].copy()\n",
    "\n",
    "    kdtree_c0 = cKDTree(c0)\n",
    "    kdtree_c1 = cKDTree(c1)\n",
    "    neighbors = kdtree_c0.query_ball_tree(kdtree_c1, neighbor_radius)\n",
    "    # neighbors1 = kdtree_c0.query_ball_point(c1,neighbor_radius) #1 is all unique idx, 2 is the location\n",
    "    # neighbors2 = kdtree_c0.sparse_distance_matrix(kdtree_c1, neighbor_radius)\n",
    "    dist2,idx2 = kdtree_c0.query(c1, k=3)\n",
    "    [Idx_unique, I] = np.unique(idx2,return_index=True) \n",
    "#     print(len(dist2[:,0])*3)\n",
    "    \n",
    "    ## find the smallest repeated value location, and delete the others\n",
    "    for i in range(Idx_unique.shape[0]):  # should repeat for only once\n",
    "    #     print(Idx_unique[i])\n",
    "        Loc_rep=np.where(idx2==Idx_unique[i])    \n",
    "    #     print(Loc_rep[0])\n",
    "        A=dist2[Loc_rep[0],Loc_rep[1]]\n",
    "        minposition = min(A)\n",
    "        Loc_min = np.where(A==minposition)[0]\n",
    "    #     print(Loc_min)\n",
    "    #     Loc_rep_min=Loc_rep[0][Loc_min[0]]    \n",
    "        Loc_rep_nouse=np.delete(range(len(Loc_rep[0])),Loc_min)\n",
    "        dist2[Loc_rep[0][Loc_rep_nouse],Loc_rep[1][Loc_rep_nouse]]=neighbor_radius*2 \n",
    "    ## find the results that are less than radius; used later column data \n",
    "    # when only first row is not exist use latter column, or just dispose it. \n",
    "    co_loc=np.where(dist2>neighbor_radius)\n",
    "\n",
    "    for j in range(dist2.shape[0]):\n",
    "         if dist2[j,0] < neighbor_radius:\n",
    "                dist2[j,1] = neighbor_radius*2\n",
    "    for j in range(dist2.shape[0]):            \n",
    "         if dist2[j,0] <neighbor_radius or dist2[j,1] <neighbor_radius:\n",
    "                dist2[j,2] = neighbor_radius*2       \n",
    "    row_c1 = np.where(dist2<neighbor_radius)\n",
    "#     print(len(row_c1[0]))\n",
    "\n",
    "    # lipo spot_c1 is row_c1\n",
    "    pBind = row_c1[0]\n",
    "    # print(idx2)\n",
    "    # lipo spot_c1 is idx2\n",
    "    pAind = [(idx2[row_c1[0][x], row_c1[1][x]]) for x in range(len(row_c1[0]))]\n",
    "    lipo_c0 = spot_c0[pAind]\n",
    "    lipo_c1 = spot_c1[pBind]\n",
    "\n",
    "#     print(np.unique(pAind).shape)\n",
    "    \n",
    "    true_pos_c0 = np.delete(spot_c0, pAind, axis=0)\n",
    "    true_pos_c1 = np.delete(spot_c1, pBind, axis=0) #true\n",
    "\n",
    "    if spot_c0.shape[0]>0:\n",
    "        P1 = (lipo_c0.shape[0] / spot_c0.shape[0])*100   # % mov spots from  previous images  /spot_c0.shape\n",
    "    else:\n",
    "        P1 = 0\n",
    "        \n",
    "    if spot_c1.shape[0]>0:\n",
    "        P2 = (lipo_c1.shape[0] / spot_c1.shape[0])*100  # % fixed spots can be found in later mov images  /spot_c0.shape\n",
    "    else:\n",
    "        P2 = 0\n",
    "\n",
    "    print('% P1: ',str(P1) + ';  % P2: ',str(P2)) \n",
    "\n",
    "    Dist = eucldist(lipo_c0,lipo_c1)\n",
    "\n",
    "    return lipo_c0,lipo_c1,true_pos_c0,true_pos_c1,Dist\n",
    "\n",
    "# compute distance of paired spot cloud\n",
    "def cloud_distance(spot_fix,spot_mov):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    c0=spot_fix[:,:3].copy()\n",
    "    c1=spot_mov[:,:3].copy()\n",
    "    kdtree_c0 = cKDTree(c0)\n",
    "    kdtree_c1 = cKDTree(c1)\n",
    "    dist2,idx2 = kdtree_c0.query(c1, k=3)\n",
    "    [Idx_unique, I] = np.unique(idx2,return_index=True) \n",
    "    return dist2[:,0]\n",
    "\n",
    "# euclidean_distances(lipo_c0,lipo_c1).shape\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "def eucldist(coords1, coords2):\n",
    "    \"\"\" Calculates the euclidean distance between 2 lists of coordinates. \"\"\"\n",
    "    dist = np.zeros(len(coords1))\n",
    "    i = 0\n",
    "    for (x, y) in zip(coords1, coords2):\n",
    "        p1 = x\n",
    "        p2 = y\n",
    "        squared_dist = (p1[0]-p2[0])**2+(p1[1]-p2[1])**2+(p1[2]-p2[2])**2\n",
    "        dist[i] = np.sqrt(squared_dist)\n",
    "        i = i+1\n",
    "    return dist\n",
    "# dist = eucldist(lipo_c0, lipo_c1)\n",
    "# print(dist)\n",
    "\n",
    "def match_points(A, B, scores, threshold):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    scores = 0\n",
    "    A1,B1,_,_,_ = colocalization(A,B,threshold)\n",
    "    # return positions of corresponding points\n",
    "    return A1,B1\n",
    "\n",
    "def violin_distance(A, B):    \n",
    "    fig=plt.figure(dpi=120,figsize=(2,3))\n",
    "    plt.violinplot(cloud_distance(A,B))\n",
    "    sns.despine() \n",
    "    plt.xlabel('Spots:'+ str(cloud_distance(A,B).shape[0]))\n",
    "    plt.ylabel('Distance/pixel')\n",
    "    ave=np.average(cloud_distance(A,B))\n",
    "    plt.title(str(float('%.2f' % ave)))\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    return ave\n",
    "\n",
    "def pair_match(spot_c0,spot_c1,neighbor_radius):\n",
    "    c0=spot_c0[:,:3].copy()\n",
    "    c1=spot_c1[:,:3].copy()\n",
    "    kdtree_c0 = cKDTree(c0)\n",
    "    kdtree_c1 = cKDTree(c1)\n",
    "    neighbors = kdtree_c0.query_ball_tree(kdtree_c1, neighbor_radius)\n",
    "    dist2,idx2 = kdtree_c0.query(c1, k=3)\n",
    "    [Idx_unique, I] = np.unique(idx2,return_index=True) \n",
    "    ## find the smallest repeated value location, and delete the others\n",
    "    for i in range(Idx_unique.shape[0]):  # should repeat only once\n",
    "        Loc_rep=np.where(idx2==Idx_unique[i])    \n",
    "        A=dist2[Loc_rep[0],Loc_rep[1]]\n",
    "        minposition = min(A)\n",
    "        Loc_min = np.where(A==minposition)[0]\n",
    "        Loc_rep_nouse=np.delete(range(len(Loc_rep[0])),Loc_min)\n",
    "        dist2[Loc_rep[0][Loc_rep_nouse],Loc_rep[1][Loc_rep_nouse]]=neighbor_radius*2 \n",
    "    ## find the results that are less than radius; used later column data \n",
    "    # use latter column when only first row is not exist, or just dispose it. \n",
    "    co_loc=np.where(dist2>neighbor_radius)\n",
    "\n",
    "    for j in range(dist2.shape[0]):\n",
    "         if dist2[j,0] < neighbor_radius:\n",
    "                dist2[j,1] = neighbor_radius*2\n",
    "    for j in range(dist2.shape[0]):            \n",
    "         if dist2[j,0] <neighbor_radius or dist2[j,1] <neighbor_radius:\n",
    "                dist2[j,2] = neighbor_radius*2       \n",
    "    row_c1 = np.where(dist2<neighbor_radius)\n",
    "    pBind = row_c1[0]\n",
    "    pAind = [(idx2[row_c1[0][x], row_c1[1][x]]) for x in range(len(row_c1[0]))]\n",
    "#     lipo_c0 = spot_c0[pAind]\n",
    "#     lipo_c1 = spot_c1[pBind]\n",
    "    Aind = spot_c0[pAind][:,3].astype(int)\n",
    "    Bind = spot_c1[pBind][:,3].astype(int)\n",
    "    return Aind,Bind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ICP\n",
    "def nearest_neighbor(src, dst):\n",
    "    '''\n",
    "    Find the nearest (Euclidean) neighbor in dst for each point in src\n",
    "    Input:\n",
    "        src: Nx3 array of points\n",
    "        dst: Nx3 array of points\n",
    "    Output:\n",
    "        distances: Euclidean distances (errors) of the nearest neighbor\n",
    "        indecies: dst indecies of the nearest neighbor\n",
    "    '''\n",
    "\n",
    "    indecies = np.zeros(src.shape[0], dtype=np.int)\n",
    "    distances = np.zeros(src.shape[0])\n",
    "    for i, s in enumerate(src):\n",
    "        min_dist = np.inf\n",
    "        for j, d in enumerate(dst):\n",
    "            dist = np.linalg.norm(s-d)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                indecies[i] = j\n",
    "                distances[i] = dist\n",
    "    return distances, indecies  \n",
    "\n",
    "def best_fit_transform(A, B):\n",
    "    '''\n",
    "    Calculates the least-squares best-fit transform between corresponding 3D points A->B\n",
    "    Input:\n",
    "      A: Nx3 numpy array of corresponding 3D points\n",
    "      B: Nx3 numpy array of corresponding 3D points\n",
    "    Returns:\n",
    "      T: 4x4 homogeneous transformation matrix\n",
    "      R: 3x3 rotation matrix\n",
    "      t: 3x1 column vector\n",
    "    '''\n",
    "    assert len(A) == len(B)\n",
    "\n",
    "    # translate points to their centroids\n",
    "    centroid_A = np.mean(A, axis=0) \n",
    "    centroid_B = np.mean(B, axis=0)\n",
    "    AA = A - centroid_A\n",
    "    BB = B - centroid_B\n",
    "\n",
    "    # rotation matrix\n",
    "    W = np.dot(BB.T, AA)\n",
    "    U, s, VT = np.linalg.svd(W, full_matrices=True, compute_uv=True)\n",
    "    R = np.dot(U, VT)\n",
    "\n",
    "    # special reflection case\n",
    "    if np.linalg.det(R) < 0:\n",
    "        VT[2,:] *= -1\n",
    "        R = np.dot(U, VT)\n",
    "\n",
    "    # translation\n",
    "    t = centroid_B.T - np.dot(R,centroid_A.T)\n",
    "\n",
    "    # homogeneous transformation\n",
    "    T = np.identity(4)\n",
    "    T[0:3, 0:3] = R\n",
    "    T[0:3, 3] = t\n",
    "\n",
    "    return T, R, t\n",
    "       \n",
    "def icp(A, B, ICP_error,init_pose = None, max_iterations=200, tolerance=0.001):\n",
    "    '''\n",
    "    The Iterative Closest Point method\n",
    "    Input:\n",
    "        A: Nx3 numpy array of source 3D points\n",
    "        B: Nx3 numpy array of destination 3D point\n",
    "        init_pose: 4x4 homogeneous transformation\n",
    "        max_iterations: exit algorithm after max_iterations\n",
    "        tolerance: convergence criteria\n",
    "    Output:\n",
    "        T: final homogeneous transformation\n",
    "        distances: Euclidean distances (errors) of the nearest neighbor\n",
    "    '''\n",
    "    #  select points \n",
    "#     ICP_error = 30\n",
    "    A,B,_,_,_ = colocalization(A,B,ICP_error)\n",
    "    \n",
    "    # make points homogeneous, copy them so as to maintain the originals\n",
    "    src = np.ones((4,A.shape[0]))  #(4, A.shape[0])\n",
    "    dst = np.ones((4,B.shape[0]))\n",
    "    src[0:3,:] = np.copy(A.T)  # A.T shape (3,20)\n",
    "    dst[0:3,:] = np.copy(B.T)\n",
    "    \n",
    "    # apply the initial pose estimation\n",
    "    if init_pose is not None:\n",
    "        src = np.dot(init_pose, src)\n",
    "\n",
    "    prev_error = 0\n",
    "    distances_iter = np.zeros((max_iterations,1))\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # find the nearest neighbours between the current source and destination points\n",
    "        distances, indices = nearest_neighbor(src[0:3,:].T, dst[0:3,:].T)\n",
    "        \n",
    "        # compute the transformation between the current source and nearest destination points\n",
    "        T,_,_ = best_fit_transform(src[0:3,:].T, dst[0:3,indices].T)  \n",
    "        # update the current source\n",
    "    # refer to \"Introduction to Robotics\" Chapter2 P28. Spatial description and transformations\n",
    "        src = np.dot(T, src)\n",
    "\n",
    "        # check error\n",
    "        mean_error = np.sum(distances) / distances.size\n",
    "#         print(f'mean_error: {mean_error}')\n",
    "#         if abs(prev_error-mean_error) < tolerance:\n",
    "#             break\n",
    "        prev_error = mean_error\n",
    "        distances_iter[i] = mean_error\n",
    "    \n",
    "    fig=plt.figure(dpi=120,figsize=(2,3))\n",
    "    plt.plot(distances_iter[:20])\n",
    "    sns.despine() \n",
    "    plt.xlabel('Spots:'+ str(distances_iter[:20].shape[0]))\n",
    "    plt.ylabel('Distance/pixel')\n",
    "    ave=np.average(distances_iter[:20])\n",
    "    plt.title(str(float('%.2f' % ave)))\n",
    "    plt.show()\n",
    "    # calculcate final tranformation\n",
    "    T,_,_ = best_fit_transform(A, src[0:3,:].T)\n",
    "\n",
    "    return T, distances     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####Get metadata for ROI: ID, centroid position, size, distance to (0,0,0) and aspect ratio####\n",
    "def ROI_meta(segmentation,out_dir):\n",
    "    lb = segmentation.astype(int)\n",
    "    roi = np.max(lb)\n",
    "    lb_id = np.unique(lb[lb != 0]) \n",
    "    df = pd.DataFrame(np.empty([lb_id.shape[0], 0]))\n",
    "    lb_stat = regionprops(lb)\n",
    "    ROI_points = np.empty([lb_id.shape[0],4])\n",
    "    i = 0\n",
    "    for j in lb_id: \n",
    "        df.loc[df.index[i], 'roi'] = j\n",
    "        df.loc[df.index[i], 'x'] = lb_stat[i].centroid[0]\n",
    "        df.loc[df.index[i], 'y'] = lb_stat[i].centroid[1]\n",
    "        df.loc[df.index[i], 'z'] = lb_stat[i].centroid[2]\n",
    "        df.loc[df.index[i], 'area'] = lb_stat[i].area\n",
    "#         df.loc[df.index[i], 'Distance'] = distance.euclidean(lb_stat[i].centroid,[0,0,0])\n",
    "        df.loc[df.index[i], 'minor_axis_length'] = lb_stat[i].minor_axis_length\n",
    "        df.loc[df.index[i], 'major_axis_length'] = lb_stat[i].major_axis_length\n",
    "        df.loc[df.index[i], 'aspect_ratio'] = lb_stat[i].minor_axis_length/lb_stat[i].major_axis_length\n",
    "        ROI_points[i,:3] = lb_stat[i].centroid\n",
    "        ROI_points[i,3] = j\n",
    "        i = i+1\n",
    "    ####Filter out ROIs that 1) not in mask; 2) have high background in channel 546\n",
    "    list_ROI = [0]\n",
    "    df_filtered=df.loc[~df['roi'].isin(list_ROI)]\n",
    "    df_filtered.to_csv(out_dir, index=False)\n",
    "    print(ROI_points.shape)\n",
    "    return df_filtered,ROI_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) correlation (corr > 0.999 in average intensity of 4 channels) \n",
    "# 2) less than 75 pixels apart(centroid position distance, for 3x expansion)\n",
    "# 3) > 4% of cell volume contacting\n",
    "# 4) for ROIs that have more than two corresponding pairs, the corresponding segments will be ranked by % of contacting, \n",
    "# and they will be flagged for manual inspection\n",
    "# 5) if both segments are bigger than 20000 in size, it will be flagged and manually inspected\n",
    "# 6) at least one of the segments have to be bigger than 7000 pixels in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsegmentation_ROI(lb, r, out_dir, count_dir, Mean_int_dir, roi_dir):\n",
    "\n",
    "# Would need to test if it is the same with Yuhan's orginal one with multiple rounds \n",
    "\n",
    "# Code used to flag over-segmentation errors for a single round. \n",
    "# A. maximize the number of identified errors\n",
    "# B. minimize false detection of well-segemented cells \n",
    "# \n",
    "# The over-segmentation pairs are identified with the following criteria: \n",
    "# 1) the two segments have a very high correlation (corr > 0.999 in average intensity of 4 channels) \n",
    "# 2) the two segments have to be less than 75 pixels apart(centroid position distance, for 3x expansion)\n",
    "# 3) The two segments have to be touching each other: at least one > 4% of cell volume contacting\n",
    "# 4) for ROIs that have more than two corresponding pairs, the corresponding segments will be ranked by % of contacting, \n",
    "# and they will be flagged for manual inspection\n",
    "# 5) if both segments are bigger than 20000 in size, it will be flagged and manually inspected\n",
    "# 6) at least one of the segments have to be bigger than 7000 pixels in size\n",
    "# #1，#2, #3, #4 are for initial identification of oversegmentation errors \n",
    "# #5, #6 are for eliminating the false detections  \n",
    "\n",
    "#     count_dir      = seg_dir + 'R2_3tm50_1920/roi_spots_all.csv' # directory to spot count/neuron (csv format)\n",
    "#     Mean_int_dir   = seg_dir + 'R2_mean_intensity.csv' # directory to mean intensity/neuron (csv format)\n",
    "# #     roi_dir        = seg_dir + 'allroi_fix.csv'\n",
    "#     lb_dir         = 'E:/Maxprobe_analysis/R2_R1_3tm50/mask2.tif' # directory to segmentation mask (tif format)\n",
    "#     out_dir        = seg_dir\n",
    "\n",
    "    df1=pd.read_csv(count_dir,sep=',', index_col=0) ## / change to count/area so that it can be comparable.\n",
    "    df2=pd.read_csv(Mean_int_dir,sep=',', index_col=0)\n",
    "    # df= pd.concat([df1, df2], axis=1)\n",
    "    df = df2\n",
    "    roi=pd.read_csv(roi_dir,sep=',',index_col=0)\n",
    "    # roi=df_filtered_fix\n",
    "\n",
    "    ###Get correlation matrix\n",
    "    corr_raw =df.T.corr()\n",
    "    s_raw = corr_raw.stack()\n",
    "    ii_raw = s_raw[np.logical_and(s_raw > 0.999, s_raw<1.0)].index.tolist()\n",
    "    test=np.asarray(ii_raw)\n",
    "    test.sort(axis=1)\n",
    "    test=np.unique(test,axis=0).astype(int)\n",
    "\n",
    "    cand={}\n",
    "    for i in range(0,len(test)):\n",
    "        a=roi.loc[test[i,0]].to_numpy()[:3]\n",
    "        b=roi.loc[test[i,1]].to_numpy()[:3]\n",
    "        dist=distance.euclidean(a,b)\n",
    "        if dist<75 and dist>0:  # for 3x expansion\n",
    "            a_area=roi.loc[test[i,0]]['area']\n",
    "            b_area=roi.loc[test[i,1]]['area']\n",
    "            if np.maximum(a_area,b_area) >7000:\n",
    "                c=corr_raw.loc[test[i,0],test[i,1]]\n",
    "                cand[i] = np.append(test[i,:],c)\n",
    "                cand[i] = np.append(cand[i],dist)\n",
    "                if np.minimum(a_area,b_area) >20000:\n",
    "                    cand[i]=np.append(cand[i], str(np.minimum(a_area,b_area)))\n",
    "                else:\n",
    "                    cand[i]=np.append(cand[i], str('--'))\n",
    "\n",
    "    m=pd.DataFrame.from_dict(data=cand, orient='index')\n",
    "    m = m.rename(columns={0:'cell_A', 1:'cell_B', 2: 'corr', 3:'dist',4: 'min_size_20000'})\n",
    "    m['cell_A'], m['cell_B'] = np.where(m['cell_A'] > m['cell_B'], [m['cell_B'],m['cell_A'] ], [m['cell_A'] , m['cell_B']])\n",
    "\n",
    "    ### % of cell volume contacting\n",
    "#     lb=imread(lb_dir)\n",
    "    lb_stat=regionprops(lb)\n",
    "    select={}            \n",
    "    ROI_row =[]\n",
    "    ROI_row_1 =[]\n",
    "    ROI_row_2 =[]\n",
    "    ROI_row_3 =[]\n",
    "    ROI_row_4 =[]\n",
    "\n",
    "    kk = 0\n",
    "    ROI_merge = {}\n",
    "    ROI_remove = []\n",
    "    for k in range(0,len(m)):\n",
    "        id1=m.iloc[k]['cell_A'].astype(np.float).copy()\n",
    "        id2=m.iloc[k]['cell_B'].astype(np.float).copy()\n",
    "        a=lb_stat[int(id1-1)].coords\n",
    "        b=lb_stat[int(id2-1)].coords\n",
    "        kdtree_a = cKDTree(a)\n",
    "        kdtree_b = cKDTree(b)\n",
    "        nnn=kdtree_a.count_neighbors(kdtree_b,1)    \n",
    "        edge_ratio_a = 100*nnn/roi.loc[id1]['area'].astype(int)\n",
    "        edge_ratio_b = 100*nnn/roi.loc[id2]['area'].astype(int)\n",
    "\n",
    "        if nnn>0:\n",
    "            select[k]=m.iloc[k]\n",
    "            # edge_ratio_a\n",
    "            if edge_ratio_a<4:\n",
    "    #             select[k]=np.append(select[k], str('less_100'))\n",
    "                select[k]=np.append(select[k], str('less_4%')) \n",
    "            else:\n",
    "                select[k]=np.append(select[k], str(edge_ratio_a)) # ('--')\n",
    "            # edge_ratio_b    \n",
    "            if edge_ratio_b<4:\n",
    "    #             select[k]=np.append(select[k], str('less_100'))\n",
    "                select[k]=np.append(select[k], str('less_4%')) \n",
    "            else:\n",
    "                select[k]=np.append(select[k], str(edge_ratio_b)) # ('--')\n",
    "\n",
    "            if (edge_ratio_b < 4) and (edge_ratio_a < 4):    \n",
    "                select[k]=np.append(select[k], str('--')) # ('--')\n",
    "            else:\n",
    "                # get all rows\n",
    "                ROI_merge1 = [int(id1),int(id2),edge_ratio_a,edge_ratio_b]\n",
    "                ROI_merge[kk] = ROI_merge1 # ('--')\n",
    "                kk = kk + 1\n",
    "                # 'cell_B' can be only merge to one 'cell_A': find the biggest edge_ratio_b;      \n",
    "                select[k]=np.append(select[k], str('merge'))\n",
    "                # 'cell_A' can be only merge to one 'cell_B': find the biggest edge_ratio_a;  \n",
    "\n",
    "    ### generate oversegmentation list\n",
    "    select=pd.DataFrame.from_dict(data=select, orient='index')\n",
    "    select = select.rename(columns={0:'cell_A', 1:'cell_B', 2: 'corr', 3:'dist',4: 'min_size_20000',5:'touch_A',6:'touch_B',7:'Merge'})\n",
    "    select.to_csv(out_dir + '/'+ r + '_oversegmentation.csv')\n",
    "\n",
    "    ### automatic merge oversegmenation masks when touching ratio > 4%\n",
    "    n=pd.DataFrame.from_dict(data=ROI_merge, orient='index')\n",
    "    n = n.rename(columns={0:'cell_A', 1:'cell_B', 2: 'touch_A', 3:'touch_B'})\n",
    "    for j in range(0,kk):    \n",
    "        # 'cell_A' can be only merge with one 'cell_B': find the biggest edge_ratio_a; \n",
    "        ROI_merge_B = n.iloc[j]['cell_B'].astype(np.int32).copy()\n",
    "        ROI_row = list(np.where(n['cell_B'] == ROI_merge_B))[0]\n",
    "        if ROI_row.shape[0] > 1:\n",
    "            ROI_row_1 = list(np.where(ROI_row != n['touch_A'][ROI_row].idxmax())[0])\n",
    "    #         print(list(ROI_row[ROI_row_1]))\n",
    "            ROI_row_2 = np.array(list(set(ROI_row_2).union(set(list(ROI_row[ROI_row_1])))))\n",
    "\n",
    "        # 'cell_B' can be only merge to'cell_A' once: find the biggest edge_ratio_b;\n",
    "        ROI_merge_A = n.iloc[j]['cell_A'].astype(np.int32).copy()\n",
    "        ROI_row = list(np.where(n['cell_A'] == ROI_merge_A))[0]\n",
    "        if ROI_row.shape[0] > 1:\n",
    "            ROI_row_3 = list(np.where(ROI_row != n['touch_B'][ROI_row].idxmax())[0])\n",
    "            ROI_row_4 = np.array(list(set(ROI_row_4).union(set(list(ROI_row[ROI_row_3])))))\n",
    "        # delete select row.\n",
    "    ROI_remove  = list(np.array(set(ROI_row_4).union(set(ROI_row_2))).tolist())\n",
    "    ROI_keep = list(np.array(set(range(kk)).difference(set(ROI_remove))).tolist())\n",
    "    ROI_select = [n.iloc[i][['cell_A','cell_B','touch_A','touch_B']] for i in list(ROI_keep)]\n",
    "    ROI_select=pd.DataFrame(ROI_select) \n",
    "    ROI_select.to_csv(out_dir + '/'+ r + '_oversegmentation_final.csv')  \n",
    "    return ROI_select\n",
    "\n",
    "def merge_ROI(lb, ROI_select):\n",
    "    # Merge list\n",
    "    A = np.array(ROI_select.values.tolist())[:,0].astype(int) # cell_A\n",
    "    B = np.array(ROI_select.values.tolist())[:,1].astype(int) # cell_B\n",
    "    C = list(np.array(set(A).intersection(set(B))).tolist()) # replicate list\n",
    "    row_C = [list(np.where(A == i)[0])[0] for i in C]  \n",
    "    lb_vary = lb.copy()\n",
    "    for i in C: #Merge replicate roi first        \n",
    "        row_A = A[np.where(A == i)]\n",
    "        row_B = B[np.where(A == i)] # find the row \n",
    "        # merge row_C = i\n",
    "        lb_vary[np.where(lb_vary==row_B)] = row_A\n",
    "        \n",
    "    # remove the row_B from list\n",
    "    A = np.delete(A, row_C, axis=0) \n",
    "    B = np.delete(B, row_C, axis=0) \n",
    "    ii = 0\n",
    "    for j in A :   \n",
    "    # merge A and B\n",
    "        lb_vary[np.where(lb_vary==B[ii])] = j\n",
    "        ii = ii +1\n",
    "    return(lb_vary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_intensity(regionmask, intensity_image):\n",
    "    return np.sum(intensity_image)\n",
    "\n",
    "def ROI_intensity(lb,puncta_path, r, df_filtered):\n",
    "    # calculate ROI mean intensity for four channels\n",
    "    roi= np.unique(lb[lb != 0])\n",
    "    df_mean = pd.DataFrame(data=np.empty([len(roi),1]), columns=['roi'], dtype=object)\n",
    "    df_integrated = pd.DataFrame(data=np.empty([len(roi),1]), columns=['roi'], dtype=object)\n",
    "    im = zarr.open(store=zarr.N5Store(puncta_path), mode='r')\n",
    "    cc = ['c0','c1','c2','c3']\n",
    "    \n",
    "    for c in cc:\n",
    "        img = im[c +'/s2'][...]\n",
    "        if c == 'c3':\n",
    "            dapi=im['c2/s2'][...]\n",
    "            lo=np.percentile(np.ndarray.flatten(dapi),99.5)\n",
    "            bg_dapi=np.percentile(np.ndarray.flatten(dapi[dapi!=0]),1)\n",
    "            bg_img=np.percentile(np.ndarray.flatten(img[img!=0]),1)\n",
    "            dapi_factor=np.median((img[dapi>lo] - bg_img)/(dapi[dapi>lo] - bg_dapi))\n",
    "            img = np.maximum(0, img - bg_img - dapi_factor * (dapi - bg_dapi)).astype('float32')\n",
    "        elif c == 'c2':\n",
    "            img = im['c2/s2'][...] \n",
    "        else:\n",
    "            dapi=im['c2/s2'][...]\n",
    "            lo=np.percentile(np.ndarray.flatten(dapi),99.5)\n",
    "            bg_dapi=np.percentile(np.ndarray.flatten(dapi[dapi!=0]),1)\n",
    "            bg_img=np.percentile(np.ndarray.flatten(img[img!=0]),1)\n",
    "            dapi_factor=0\n",
    "            img = np.maximum(0, img - bg_img - dapi_factor * (dapi - bg_dapi)).astype('float32') \n",
    "\n",
    "        lb_stat = regionprops(lb,intensity_image=img,extra_properties=(integrated_intensity,))\n",
    "        for i in range(0,len(roi)):\n",
    "            df_mean.loc[i, 'roi'] = lb_stat[i].label\n",
    "            df_integrated.loc[i, 'roi'] = lb_stat[i].label\n",
    "            df_mean.loc[i, r+'_'+c+'_mean_intensity'] = lb_stat[i].mean_intensity\n",
    "            df_integrated.loc[i, r+'_'+c+'_integrated_intensity'] = lb_stat[i].integrated_intensity  \n",
    "            \n",
    "    df_mean.to_csv(seg_dir + r + '_mean_intensity.csv', index=False)\n",
    "    df_integrated.to_csv(seg_dir + r + '_integrated_intensity.csv', index=False)\n",
    "    return df_mean,df_integrated\n",
    "\n",
    "def non_neuron_ROI(df_mean,df_filtered,r):    \n",
    "    \n",
    "    # calculate ROI mean intensity for four channels\n",
    "    # remove low expressor of C0C3 ratio\n",
    "    # remove small area 1.5*ratio\n",
    "    \n",
    "    print('ROI mean intensity for channels:c0-c3')\n",
    "    ratio = 2\n",
    "#     df_integrated = pd.read_csv(seg_dir + r + '_integrated_intensity.csv',sep=',', index_col=0)\n",
    "    n,b=np.histogram(df_mean[r+'_c0_mean_intensity'].values, bins=5000)    \n",
    "    thres0=b[np.argwhere(n == n.max())][0][0]*ratio\n",
    "    print(thres0)\n",
    "    n,b=np.histogram(df_mean[r+'_c1_mean_intensity'].values, bins=5000)    \n",
    "    thres1=b[np.argwhere(n == n.max())][0][0]*ratio\n",
    "    print(thres1)\n",
    "    n,b=np.histogram(df_mean[r+'_c2_mean_intensity'].values, bins=5000)    \n",
    "    thres2=b[np.argwhere(n == n.max())][0][0]*ratio\n",
    "    print(thres2)\n",
    "    n,b=np.histogram(df_mean[r+'_c3_mean_intensity'].values, bins=5000)    \n",
    "    thres3=b[np.argwhere(n == n.max())][0][0]*ratio\n",
    "    print(thres3)\n",
    "    ROI_c0 = np.where(df_mean[r+'_c0_mean_intensity'].values < thres0)[0]\n",
    "    ROI_c1 = np.where(df_mean[r+'_c1_mean_intensity'].values < thres1)[0]\n",
    "    ROI_c2 = np.where(df_mean[r+'_c2_mean_intensity'].values < thres2)[0]\n",
    "    ROI_c3 = np.where(df_mean[r+'_c3_mean_intensity'].values < thres3)[0]\n",
    "\n",
    "    ROI_c0 = df_mean['roi'][ROI_c0].tolist()\n",
    "    ROI_c1 = df_mean['roi'][ROI_c1].tolist()\n",
    "    ROI_c2 = df_mean['roi'][ROI_c2].tolist()\n",
    "    ROI_c3 = df_mean['roi'][ROI_c3].tolist()\n",
    "    ROI_c3_c0 = np.array(list(set(ROI_c0).intersection(set(ROI_c3)))) # intersection？ \n",
    "    # print(ROI_c3_c0.shape[0])\n",
    "    ROI_cx = ROI_c3_c0\n",
    "#     ROI_cx = np.array(list(set(ROI_c3_c0).intersection(set(ROI_c2))))\n",
    "    print('lowerexpressorinc0c3:' + str(len(ROI_c3_c0)))\n",
    "    \n",
    "    # remove small area\n",
    "    n,b=np.histogram(df_filtered['area'].values, bins=5000)    \n",
    "    thres=b[np.argwhere(n == n.max())][0][0]*1.5*ratio\n",
    "    print('areas>'+ str(np.round(thres)))\n",
    "    ROI_area =np.where(df_filtered['area'] < thres)[0]\n",
    "    ROI_area = df_filtered['roi'][ROI_area].tolist()\n",
    "    print('smallroi:' + str(len(ROI_area)))\n",
    "    \n",
    "    ROI_non_neuron = np.array(list(set(ROI_cx).union(set(ROI_area))))\n",
    "    print('non_neuron_ROI:' + str(len(ROI_non_neuron))) \n",
    "    \n",
    "    return ROI_non_neuron,ROI_cx,ROI_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROI_spot(out_dir,lb,spot_dir,rr,s):\n",
    "    \n",
    "#     roi_dir            = out_dir + 'roi_all.csv'   # directory to file containing the ROI metadata (neuron volume, etc.)\n",
    "#     spot_dir           = out_dir + 'R1_3tm50_1920/*.txt'  # directory to folder of airlocalize output (1 file/gene, txt format)\n",
    "#     spotcount_dir      = out_dir + 'R1_3tm50_1920/roi_spots_all.csv'   # directory to assignedhen spots per neuron based on airlocalize (csv format)\n",
    "#     output_dir         = out_dir + 'R1_3tm50_1920/Dense_spotcount.csv' # directory where output should be stored\n",
    "    \n",
    "    ## found spot with the roi \n",
    "    # lb = imread(lb_dir)      \n",
    "    lb_id = np.unique(lb[lb != 0])\n",
    "    z, y, x = lb.shape\n",
    "    a = 0\n",
    "#     s = [0.92,0.92,0.84]  ## voxel size in segmentation image [0.23, 0.23, 0.42] \n",
    "    count = pd.DataFrame(np.empty([len(lb_id), 0]), index=lb_id)\n",
    "    fx=sorted(glob(spot_dir))     # in um\n",
    "    for f in fx:\n",
    "        r=os.path.basename(f).split('.')[0]\n",
    "        spot=np.loadtxt(f, delimiter=',')                  \n",
    "        print(\"Load:\", f)\n",
    "        # convert from physical unit to pixel unit \n",
    "        rounded_spot = np.round(spot[:, :3]/s).astype('int')    ############## every spot location in pixel\n",
    "        df = pd.DataFrame(np.zeros([len(lb_id), 1]),\n",
    "                          index=lb_id, columns=['count'])    ## only for count\n",
    "        spot=np.append(spot, np.zeros((len(spot),1)), axis=1)  ############ every spot location in um ## add last column    \n",
    "        n = len(spot)\n",
    "        for i in range(0, n):                 ## lb = all the valid ROI\n",
    "            if np.any(np.isnan(spot[i,:3])):  ## spot[i,:3]   ##if spot located outside of segmentation image\n",
    "                print('NaN found in {} line# {}'.format(f, i+1))\n",
    "            else:\n",
    "                if np.any(spot[i,:3]<0):\n",
    "                    a = a + 1\n",
    "                else:\n",
    "                    try:\n",
    "                        Coord = rounded_spot[i]\n",
    "                        idx = lb[Coord[2]-1, Coord[1]-1, Coord[0]-1]   # roi id                     \n",
    "                        if idx > 0 and idx <= np.max(lb_id):\n",
    "                            df.loc[idx, 'count'] = df.loc[idx, 'count']+1\n",
    "                            spot[i,4]=idx  # add ROI number                                       \n",
    "                    except Exception as e:  # outside of ROIs\n",
    "                        a = a + 1\n",
    "\n",
    "        spot_int=spot[spot[:,4]!=0]\n",
    "        print(spot_int.shape)\n",
    "        np.savetxt(out_dir  + 'RS-FISH/' + r +'_ROI.txt'.format(r),spot_int,delimiter=',')  \n",
    "        ## found count with the roi_num  \n",
    "        count.loc[:, r] = df.to_numpy() # r\n",
    "\n",
    "    out_dir1 = out_dir + 'RS-FISH/' + rr + '_spots_all.csv'\n",
    "    count.to_csv(out_dir1)\n",
    "    # also save to txt.\n",
    "    # spotcount=pd.read_csv(spotcount_dir,sep=',', index_col=0)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter segmented ROIs for fixed round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### load masks and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Later analysis for generate ROI_spots once the extracted spots has been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load: E:/Maxprobe_analysis/R2_R1_3tm50/RS-FISH\\R1_c0_warped.txt\n",
      "(127022, 5)\n",
      "Load: E:/Maxprobe_analysis/R2_R1_3tm50/RS-FISH\\R1_c3_warped.txt\n",
      "(143752, 5)\n",
      "(340, 4)\n",
      "(340, 4)\n",
      "CPU times: total: 49.5 s\n",
      "Wall time: 51.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import all packages\n",
    "from glob import glob\n",
    "from skimage.io import imread, imsave\n",
    "seg_dir='E:/Maxprobe_analysis/R2_R1_3tm50/' # \n",
    "r = ['R2','R1'][1]\n",
    "s = [0.92,0.92,0.84]\n",
    "\n",
    "#RS-FISH\n",
    "for i in [0,1]:\n",
    "    segmentation = imread(seg_dir + 'R2' + '_filtered_mask.tif')\n",
    "    spot_dir = seg_dir + 'RS-FISH/'+ r[i] + '*warped.txt'\n",
    "#     spot_dir = seg_dir + 'RS-FISH/'+ r[i] + '_c3.txt'\n",
    "    count = ROI_spot(seg_dir,segmentation,spot_dir,r[i],s)   \n",
    "    out_dir = seg_dir + 'RS-FISH/'+ r[i] + '_allroi.csv'\n",
    "    df_filtered,_ = ROI_meta(segmentation,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam the spot position after bigstream registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI: #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhenggang\\Anaconda3\\envs\\easifish\\lib\\site-packages\\numpy\\core\\numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n",
      "C:\\Users\\zhenggang\\Anaconda3\\envs\\easifish\\lib\\site-packages\\numpy\\core\\numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n",
      "C:\\Users\\zhenggang\\Anaconda3\\envs\\easifish\\lib\\site-packages\\numpy\\core\\numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n",
      "C:\\Users\\zhenggang\\Anaconda3\\envs\\easifish\\lib\\site-packages\\numpy\\core\\numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Points layer 'bigstream_c3' at 0x1e7abe66a90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewer = napari.view_image(data.astronaut(), rgb=True)\n",
    "# napari.run()\n",
    "spot_extraction = ['hAir/','RS-FISH/'][1]\n",
    "spot_fix_c3_all = np.loadtxt (seg_dir + spot_extraction + 'R2_c3_ROI.txt',delimiter=',')\n",
    "spot_fix_c0_all = np.loadtxt (seg_dir + spot_extraction + 'R2_c0_ROI.txt',delimiter=',')\n",
    "warp_spots_c3_all = np.loadtxt (seg_dir + spot_extraction + 'R1_c3_warped_ROI.txt',delimiter=',')\n",
    "warp_spots_c0_all = np.loadtxt (seg_dir + spot_extraction + 'R1_c0_warped_ROI.txt',delimiter=',')\n",
    "fix_spacing = np.array([0.42,0.23,0.23])\n",
    "\n",
    "aa = 5\n",
    "print('ROI: #' + str(aa))\n",
    "fix_c3 = spot_fix_c3_all[spot_fix_c3_all[:,4] == aa][:,:3]\n",
    "fix_c0 = spot_fix_c0_all[spot_fix_c0_all[:,4] == aa][:,:3]\n",
    "warp_c3 = warp_spots_c3_all[warp_spots_c3_all[:,4]== aa][:,:3]\n",
    "warp_c0 = warp_spots_c0_all[warp_spots_c0_all[:,4]== aa][:,:3]\n",
    "\n",
    "# change to zyx order\n",
    "fix_c3 = np.transpose(np.array([fix_c3[:,2],fix_c3[:,1],fix_c3[:,0]]))\n",
    "warp_c3 = np.transpose(np.array([warp_c3[:,2],warp_c3[:,1],warp_c3[:,0]]))\n",
    "fix_c0 = np.transpose(np.array([fix_c0[:,2],fix_c0[:,1],fix_c0[:,0]]))\n",
    "warp_c0 = np.transpose(np.array([warp_c0[:,2],warp_c0[:,1],warp_c0[:,0]]))\n",
    "\n",
    "s=fix_c0[:,:3]/fix_spacing#convert spot um to pixel coordinates\n",
    "viewer.add_points(np.transpose(np.array([s[:,0],s[:,1],s[:,2]])),name ='fix_c0', size=3,\n",
    "                  face_color='magenta',edge_color='magenta',blending='opaque')\n",
    "s=warp_c0[:,:3]/fix_spacing#convert spot um to pixel coordinates\n",
    "viewer.add_points(np.transpose(np.array([s[:,0],s[:,1],s[:,2]])),name ='bigstream_c0', size=3,\n",
    "                  face_color='green',edge_color='green',blending='opaque')\n",
    "\n",
    "s=fix_c3[:,:3]/fix_spacing#convert spot um to pixel coordinates\n",
    "viewer.add_points(np.transpose(np.array([s[:,0],s[:,1],s[:,2]])),name ='fix_c3', size=3,\n",
    "                  face_color='magenta',edge_color='magenta',blending='opaque')\n",
    "s=warp_c3[:,:3]/fix_spacing#convert spot um to pixel coordinates\n",
    "viewer.add_points(np.transpose(np.array([s[:,0],s[:,1],s[:,2]])),name ='bigstream_c3', size=3,\n",
    "                  face_color='green',edge_color='green',blending='opaque')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After ICP affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI: #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhenggang\\Anaconda3\\envs\\easifish\\lib\\site-packages\\numpy\\core\\numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n",
      "C:\\Users\\zhenggang\\Anaconda3\\envs\\easifish\\lib\\site-packages\\numpy\\core\\numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n",
      "C:\\Users\\zhenggang\\Anaconda3\\envs\\easifish\\lib\\site-packages\\numpy\\core\\numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n",
      "C:\\Users\\zhenggang\\Anaconda3\\envs\\easifish\\lib\\site-packages\\numpy\\core\\numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Points layer 'warp_c3' at 0x1e7b61c6b50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewer = napari.view_image(data.astronaut(), rgb=True)\n",
    "# napari.run()\n",
    "Rounds = ['R2','R1']\n",
    "chn = ['c3','c0']\n",
    "spot_extraction = ['hAir/','RS-FISH/affine_cca/'][1]\n",
    "\n",
    "spot_fix_c3_all = np.loadtxt (seg_dir + spot_extraction + 'ROIaffine_'+ Rounds[0] + '_' + chn[0] + '_spots.txt',delimiter=',')\n",
    "spot_fix_c0_all = np.loadtxt (seg_dir + spot_extraction + 'ROIaffine_'+ Rounds[0] + '_' + chn[1] + '_spots.txt',delimiter=',')\n",
    "warp_spots_c3_all = np.loadtxt (seg_dir + spot_extraction + 'ROIaffine_'+ Rounds[1] + '_' + chn[0] + '_spots.txt',delimiter=',')\n",
    "warp_spots_c0_all = np.loadtxt (seg_dir + spot_extraction + 'ROIaffine_'+ Rounds[1] + '_' + chn[1] + '_spots.txt',delimiter=',')\n",
    "\n",
    "fix_spacing = np.array([0.42,0.23,0.23])\n",
    "\n",
    "aa = 5\n",
    "print('ROI: #' + str(aa))\n",
    "fix_c3 = spot_fix_c3_all[spot_fix_c3_all[:,3] == aa][:,:3]*fix_spacing\n",
    "fix_c0 = spot_fix_c0_all[spot_fix_c0_all[:,3] == aa][:,:3]*fix_spacing\n",
    "warp_c3 = warp_spots_c3_all[warp_spots_c3_all[:,3]== aa][:,:3]*fix_spacing\n",
    "warp_c0 = warp_spots_c0_all[warp_spots_c0_all[:,3]== aa][:,:3]*fix_spacing\n",
    "\n",
    "s=fix_c0[:,:3]/fix_spacing#convert spot um to pixel coordinates\n",
    "viewer.add_points(np.transpose(np.array([s[:,0],s[:,1],s[:,2]])),name ='fix_c0', size=3,\n",
    "                  face_color='magenta',edge_color='magenta',blending='opaque')\n",
    "s=warp_c0[:,:3]/fix_spacing#convert spot um to pixel coordinates\n",
    "viewer.add_points(np.transpose(np.array([s[:,0],s[:,1],s[:,2]])),name ='warp_c0', size=3,\n",
    "                  face_color='green',edge_color='green',blending='opaque')\n",
    "\n",
    "s=fix_c3[:,:3]/fix_spacing#convert spot um to pixel coordinates\n",
    "viewer.add_points(np.transpose(np.array([s[:,0],s[:,1],s[:,2]])),name ='fix_c3', size=3,\n",
    "                  face_color='magenta',edge_color='magenta',blending='opaque')\n",
    "s=warp_c3[:,:3]/fix_spacing#convert spot um to pixel coordinates\n",
    "viewer.add_points(np.transpose(np.array([s[:,0],s[:,1],s[:,2]])),name ='warp_c3', size=3,\n",
    "                  face_color='green',edge_color='green',blending='opaque')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.02102984e+01, 7.47849514e+02, 8.81808216e+02, 1.00000000e+00],\n",
       "       [5.78701982e+01, 7.35393913e+02, 8.72666916e+02, 1.00000000e+00],\n",
       "       [5.79318982e+01, 7.49952414e+02, 8.71985316e+02, 1.00000000e+00],\n",
       "       ...,\n",
       "       [1.03905667e+03, 1.26312072e+03, 1.07460102e+02, 1.12500000e+03],\n",
       "       [1.03381707e+03, 1.26856492e+03, 1.09295002e+02, 1.12500000e+03],\n",
       "       [1.03151917e+03, 1.27031922e+03, 1.17086602e+02, 1.12500000e+03]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_fix_c3_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easifish",
   "language": "python",
   "name": "easifish"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
